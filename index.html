<!DOCTYPE html>
<html>
  <head>
    <title>From zero to neural likelihood free inference.</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="./assets/katex.min.css">
    <link rel="stylesheet" href="./assets/style.css">
    <link rel="stylesheet" href="./assets/grid.css">
    <style type="text/css">
      /* Slideshow styles */
    </style>
  </head>
  <body>
    <textarea id="source">

class: left, middle

# Posterior estimation with neural networks


.left[BSc Machine learning course 2020]

.right[
Christoph Weniger, GRAPPA

University of Amsterdam

28 September 2020
]

---

# All measurements are uncertain



.grid[
.kol-1-2[
## Cosmological parameters
- Matter content vs dark energy in the Universe
- Different measurements constrain the parameters in complementary ways
<br>
<br>
<br>

.red[.center[How can we describe these uncertainties mathematically?]]
]
.kol-1-2[
.center[.width-100[![](figures/eso0419d.jpg)]]
]
]


.footnote[Credit: ESO]

---

# Probability distributions

.center.red[The distribution of continuous random numbers is described by probability density functions]

.grid[
.kol-1-2[
.center[1-dim standard normal distribution]
.width-100[![](figures/Standard_deviation_diagram.svg.png)]
]
.kol-1-2[
.center[Example for a 2-dim distribution]
.width-80[![](figures/2-dim-prob.png)]
]
]


**Bayesian statistics**: Those distributions can also describe our **belief** (plausibility/probability) that a certain parameter has a certain value.  

**Examples**: Mass of the higgs particle, age of the Universe, average temperature of Earth atmosphere in 1900 (reconstruction) or in 2100 (extrapolation).


---

# Two schools: Bayesian* vs Frequentist

.center[.width-50[![](figures/cartoon_guide_bayesian.png)]]

*what we do here

.footnote[Source: The Cartoon Guide to Statistics]

---

# Bayes' theorem 

.center.red[Bayes' theorem provides a clear rule for how to update believes with new data]

.grid[
.kol-1-2[
## Likelihood $P(D|H)$
How probable is the data $D$ given that our hypothesis $H$ is true?
]
.kol-1-2[
## Prior $P(H)$
How probable was our hypothesis $H$) before observing the evidence?
]
]

$$
P(H|D) = \frac{P(D|H) P(H)}{P(D)}
$$

--

count: false

<br>
.grid[
.kol-1-2[
## Posterior $P(D|H)$
How probably is our hypothesis $H$ given the observed data $D$?
]
.kol-1-2[
## Marginal likelihood $P(D)$
How probable is the new data $D$ under all possible hypothesis $H$?

$$
P(D) \equiv \sum_H P(D|H) P(H)
$$
]
]

---

# Update rule in practice

.grid[
.kol-1-3[
- Prior Beliefs: $P(H)$
- Evidence/likelhood: $P(D|H)$
- Posterior beliefs: $P(H|D)$
]
.kol-2-3[
.center[]
.width-100[![](figures/Bayes_update.jpg)]
]
]

--

count: false

.center.red[However, in many cases the likelihood of observing $D$ given hypothesis $H$, $P(D|H)$ is not actually known, or very hard to calculate.]

---

class: center, middle, black-slide

# Observations are often the autcome of a LARGE number of random processes

<iframe width="600" height="450" src="https://www.youtube.com/embed/Kq7e6cj2nDw" frameborder="0" allowfullscreen></iframe>


---

# Physics examples - gamma rays 

.grid[
.kol-1-2[
.center[Gamma-ray sky]
.width-100[![](figures/fermi _lat_sky.jpeg)]
]
.kol-1-2[
.center[Observed gamma-ray sources]
.width-100[![](figures/fermi3fglait.png)]
]
]

--
count:false

## Difficult to determine properties of source populations?  E.g.:

- What is the spatial distribution of pulsars in the Galactic disk?

--
count: false

- What is the luminosity function of blazars at high Galactic latitudes?

--
count: false

- Are gaps in the source map physical or due to too large noise in the image?

---

# Physics examples - strong lensing

.grid[
.kol-1-2[
.center[Strong gravitational lensing due to dark and visible matter]
.width-100[![](figures/NASA_lensing.jpg)]
]
.kol-1-2[
.center[Example: Distant galaxy lensed by red lens galaxy along the line-of-sight]
.width-100[![](figures/strong_lensing.jpeg)]
]
]

--
count: false

## Questions

- How does the unperturbed source look like and how the lens?

--
count: false


- What can we learn about the nature of dark matter?  Small clumps of dark matter would lead to characteristic distortions in the image.

---

# Physics examples - collider physics


.grid[
.kol-1-2[
.center[Illustration of collition at ALTAS detector]
.width-100[![](figures/ATLAS-figaux_01.png)]
]
.kol-1-2[
.center[Invariant mass of 4-lepton channel]
.width-90[![](figures/ATLASHIGGS-FIG1.png)]
]
]

--
count: false


## Typical questions are:
- What processes have most likely contributed to the 4-lepton signal?

--
count: false


- How does this constraint the Higgs production cross section?





---


# The likelihood-to-evidence ratio

In order to evaluate the probability of any outcome, we often have to sum or integrate over a very large number of random variables, here called $z$, that we are not actually interested in.
$$
P(D|H) = \underbrace{\int dz}_\text{\color{red} intractable}P(D|H, z)
P(z|H)
$$

--
count: false

## Examples

- Position of individual point sources on the sky

--
count: false

- Position of individual dark matter halos, individual galaxies images

--
count: false

- Physical mechanisms that lead to specific 4-lepton invariant mass

--
count: false

.red.center[There are many ways to solve the integral approximately. Here, we will use neural networks to help us out.]

---

# Neural likelihood-free inference


.bold[Starting point]: for any pair of observation $x$ and model parameter $\theta$, the goal is to estimate the probability that this pair belongs one of the following classes:
.grid[
.kol-1-2[
$H_0$: Data $x$ comes from model $\theta$

$H_1$: Data $x$ and model $\theta$ are unrelated
]
.kol-1-2[
$(x, \theta) \sim P(x, \theta) = P(x|\theta)P(\theta)$

$(x, \theta) \sim P(x)P(\theta)$
]
]

---

# Joint vs marginal samples

Examples for $H_0$, jointly sampled from $D, H \sim P(D|H) P(H)$

.grid[
.kol-1-6[
.center[Cat]
.width-100[![](figures/cat.jpeg)]
]
.kol-1-6[
.center[Donkey]
.width-100[![](figures/donkey.jpeg)]
]
.kol-1-6[
.center[Cat]
.width-100[![](figures/cat.jpeg)]
]
.kol-1-6[
.center[Cat]
.width-100[![](figures/cat.jpeg)]
]
.kol-1-6[
.center[Donkey]
.width-100[![](figures/donkey.jpeg)]
]
.kol-1-6[
.center[Donkey]
.width-80[![](figures/donkey.jpeg)]
]
]

Examples for $H_1$, marginally sampled from $D, H \sim P(D) P(H)$

.grid[
.kol-1-6[
.center[Cat]
.width-100[![](figures/donkey.jpeg)]
]
.kol-1-6[
.center[Cat]
.width-100[![](figures/cat.jpeg)]
]
.kol-1-6[
.center[Cat]
.width-100[![](figures/donkey.jpeg)]
]
.kol-1-6[
.center[Donkey]
.width-100[![](figures/donkey.jpeg)]
]
.kol-1-6[
.center[Donkey]
.width-100[![](figures/cat.jpeg)]
]
.kol-1-6[
.center[Cat]
.width-80[![](figures/cat.jpeg)]
]
]

---

# Loss function

.bold[Strategy:] We train a neural network $d_\phi(x, \theta)$ as binary classifier to estimate the probability of hypothesis $H_0$ ($y=0$) or $H_1$ ($y=1$).

The corresponding loss function (see logistic regression example) is

$$
L\left[d(x, \theta)\right] = \int dx d\theta  \left[ p(x, \theta) \ln\left(d(x, \theta)\right) + p(x)p(\theta) \ln\left(1-d(x, \theta)\right) \right]
$$

--
count: false

Minimizing that function (blackboard!) yields
$$
d(x, \theta) = \frac{P(x, \theta)}{P(x, \theta) + P(x)P(\theta)}
$$

--
count: false

Since
$$
r(x,\theta) \equiv \frac{P(x|\theta)}{P(x)} = \frac{P(\theta|x)}{P(\theta)} = \frac{1}{d(x, \theta)}-1.
$$
we are actually performing ratio estimation.

---

# Goal is to train something like this

Read world example for the analysis of gravitational wave signals

.center[.width-60[![](https://i.imgur.com/jgHhhgb.png)]]

- Goal is to train NN as function:  $d(D, H) \in (0, 1)$
- $d(D, H)$ is the probability that $D$ and $H$ are jointly drawn.


---

# Exercise: Neural posterior estimation

Your task is to take the solution to [exercise 3](#ex3) and replace point estimation with posterior estimation. To this end you have to

- Adopt the convolutional neural network such that it takes an additional
input, the radius $r$, and such that the output is between zero and one.  A
good way to do that is to replace the last layer with 
.center[$\texttt{(INPUT, r)} \to \texttt{FC} \to \texttt{RELU} \to \texttt{FC} \to \texttt{SIGMOID}\to\texttt{OUTPUT}$]
Here,
$\texttt{INPUT}$ is the output of the second-to-last $\texttt{FC}$, which is
concatenated with the input variable $r$.

- Replace the loss function by the above binary cross-entropy loss function $L[d(x, \theta)]$.

Once this is done

- Train the network and show that the posterior (which is $\propto \exp(-r(x, \theta))$), peaks usually close to the true value
- Explore how the posterior becomes broaders when making the signal model more complicated (e.g. by masking random regions).




    </textarea>
    <script src="./assets/remark-latest.min.js"></script>
    <script src="./assets/katex.min.js"></script>
    <script src="./assets/auto-render.min.js"></script>
    <script type="text/javascript">
      var options = {ratio:'14:9'};
      var renderMath = function() {
        renderMathInElement(document.body);
        // or if you want to use $...$ for math,
        renderMathInElement(document.body, {delimiters: [ // mind the order of delimiters(!?)
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\[", right: "\\]", display: true},
            {left: "\\(", right: "\\)", display: false},
        ]});
      }
      var slideshow = remark.create(options, renderMath);
    
    </script>
  </body>
</html>
