<!DOCTYPE html>
<html>
  <head>
    <title>From zero to neural likelihood free inference.</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="./assets/katex.min.css">
    <link rel="stylesheet" href="./assets/style.css">
    <link rel="stylesheet" href="./assets/grid.css">
    <style type="text/css">
      /* Slideshow styles */
    </style>
  </head>
  <body>
    <textarea id="source">

class: left, middle

# Marignal posteriors from neural networks

.center.width-40[![](figures/lec2/title.png)]

.left[BSc Machine learning course]

.right[
Christoph Weniger, GRAPPA

University of Amsterdam

28 September 2020
]

---

# Physics examples - point sources

## Questions
- What is the detection threshold?
- What is the luminosity function?
- ...

---

# Physics examples - strong lensing

## Questions
- What is the radius of the Einstein ring?
- What is the brightness of the source?
- What is the density of foreground sources?

---

# Physics examples - collider physics

## Questions
- What is the most likely production path of a signal?

---

# Bayes theorem

Bayes theorem connects likelihood, prior, posterior and evidence
$$
P(\theta|x) = \frac{P(x|\theta) P(\theta)}{P(x)}
$$

TODO: Discuss marginal parameters

---

# Graphical models

TODO: Plots for the above examples

---

class: center, middle, black-slide

<iframe width="600" height="450" src="https://www.youtube.com/embed/Kq7e6cj2nDw" frameborder="0" allowfullscreen></iframe>

---

# How to calculate paths analytically


---

# The likelihood-to-evidence ratio

In order to evaluate the probability of any outcome, we have to sum or integrate over all possible paths that could have led to this outcome
$$
P(x|\theta) = \underbrace{\int dz}_\text{\color{red} intractable}  \delta(x - z(t)) P(z|\theta)
$$

--

count: false

Even if we could estimate the shape of $P(x|\theta)$ for paths that lead to
some actually observed values of $x$, in order to get the normalization of $P(x)$ we have to
know also all paths $z$ that do __not__ lead to $x$.

--
count: false

It turns out that often the likelihood **ratios** are easier to estimate,
since normalizing factors drop out. One particular combination is the
.bold[likelihood-to-evidence ratio]:
$$
r(x, \theta) \equiv \frac{P(x|\theta)}{P(x)}
$$

.bold[Goal]: Train a neural network to approximate $r(x, \theta)$.

???

Calculating the likelihood is often .red[untractable], due to large number of internal parameters
$$
P(x|\theta) = \underbrace{\int \int dz\, dy}_\text{\color{red} untractable} \, P(x|z, y, \theta) P(z|y) P(y) P(\theta)
$$

---

# Neural likelihood-free inference


.bold[Starting point]: for any pair of observation $x$ and model parameter $\theta$, the goal is to estimate the probability that this pair belongs one of the following classes:
.grid[
.kol-1-2[
$H_0$: Data $x$ comes from model $\theta$

$H_1$: Data $x$ and model $\theta$ are unrelated
]
.kol-1-2[
$(x, \theta) \sim P(x, \theta) = P(x|\theta)P(\theta)$

$(x, \theta) \sim P(x)P(\theta)$
]
]

.red[Note]: The likelihood ratio for these two hypothesis is our function of interest, $r(x, \theta)$.

---

# Loss function

.bold[Strategy:] We train a neural network $d_\phi(x, \theta)$ as binary classifier to estimate the probability of hypothesis $H_0$ ($y=0$) or $H_1$ ($y=1$).

The corresponding loss function (see logistic regression example) is

$$
L\left[d(x, \theta)\right] = \int dx d\theta  \left[ p(x, \theta) \ln\left(d(x, \theta)\right) + p(x)p(\theta) \ln\left(1-d(x, \theta)\right) \right]
$$

--
count: false

Minimizing that function (blackboard!) yields
$$
d(x, \theta) = \frac{P(x, \theta)}{P(x, \theta) + P(x)P(\theta)}
$$

--
count: false

One can show that
$$
r(x,\theta) \equiv \frac{P(x|\theta)}{P(x)} = \frac{1}{d(x, \theta)}-1.
$$

---

# Exercise: Neural posterior estimation

Your task is to take the solution to [exercise 3](#ex3) and replace point estimation with posterior estimation. To this end you have to

- Adopt the convolutional neural network such that it takes an additional
input, the radius $r$, and such that the output is between zero and one.  A
good way to do that is to replace the last layer with 
.center[$\texttt{(INPUT, r)} \to \texttt{FC} \to \texttt{RELU} \to \texttt{FC} \to \texttt{SIGMOID}\to\texttt{OUTPUT}$]
Here,
$\texttt{INPUT}$ is the output of the second-to-last $\texttt{FC}$, which is
concatenated with the input variable $r$.

- Replace the loss function by the above binary cross-entropy loss function $L[d(x, \theta)]$.

Once this is done

- Train the network and show that the posterior (which is $\propto \exp(-r(x, \theta))$), peaks usually close to the true value
- Explore how the posterior becomes broaders when making the signal model more complicated (e.g. by masking random regions).




    </textarea>
    <script src="./assets/remark-latest.min.js"></script>
    <script src="./assets/katex.min.js"></script>
    <script src="./assets/auto-render.min.js"></script>
    <script type="text/javascript">
      var options = {ratio:'14:9'};
      var renderMath = function() {
        renderMathInElement(document.body);
        // or if you want to use $...$ for math,
        renderMathInElement(document.body, {delimiters: [ // mind the order of delimiters(!?)
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\[", right: "\\]", display: true},
            {left: "\\(", right: "\\)", display: false},
        ]});
      }
      var slideshow = remark.create(options, renderMath);
    
    </script>
  </body>
</html>
